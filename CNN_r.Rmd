---
title: "MNIST_CNN"
author: "Chengyan Ji"
date: '2023-01-20'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# BiocManager::install(version = "3.16")
```


```{r}
BiocManager::install("EBImage")
```

## R Markdown

```{r}
# packages
library(magrittr)
library(MASS)
library(class)
library(ggplot2)
library(caret)
library(kohonen)

library(keras)
library(EBImage)
library(stringr)
library(pbapply)
```

## Including Plots

Fetch data

```{r}
df0_train <- read.csv("C:/Users/KurtJi/OneDrive - University of Illinois - Urbana/Desktop/Personal Projects/Image_classification/fashion_mnist/fashion-mnist_train.csv")
df0_test <- read.csv("C:/Users/KurtJi/OneDrive - University of Illinois - Urbana/Desktop/Personal Projects/Image_classification/fashion_mnist/fashion-mnist_test.csv")

df0_train_y <- df0_train[,1]
df0_train_x <- df0_train[,-1]

df0_test_y <- df0_test[,1]
df0_test_x <- df0_test[,-1]
```

```{r}
# convert to matrix so keras can use
df_train_x <- as.matrix(df0_train_x, 28, 28)
df_test_x <- as.matrix(df0_test_x, 28, 28)
```

```{r}
# df_train_x <- df0_train %>% select(-label) %>% as.matrix() %>% 
#   array_reshape(dim = c(nrow(df0_train), 28, 28, 1)) / 255
# df_train_y <- df0_train$label
# 
# df_test_x <- df0_test %>% select(-label) %>% as.matrix() %>% 
#   array_reshape(dim = c(nrow(df0_train), 28, 28, 1)) / 255
# df_test_y <- df0_test$label
```

```{r}
df_train_x <- array_reshape(df_train_x, c(nrow(df_train_x), 28, 28, 1))
df_test_x <- array_reshape(df_test_x, c(nrow(df_test_x), 28, 28, 1))

df_train_y <- to_categorical(df_train_y, 10)
df_test_y <- to_categorical(df_test_y, 10)
```

```{r}
# visualize sample plots
clockwise90 = function(a) {t(a[nrow(a):1,])}
```

```{r}
# visualize sample plots
par(mfrow = c(3,5))

names = c('T-shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')
for (i in 0:9) {
  temp = df_train[df_train[,1] == i, ]
  cloth_1 <- as.matrix(temp[1,-1])
  cloth_1 <- matrix(cloth_1, nrow = 28, ncol = 28)
  cloth_1 <- clockwise90(clockwise90(cloth_1))
  image(cloth_1, col = gray.colors(25), axes = FALSE, main = paste(i, ':', names[i+1]))
}
```
blah blah
```{r}
# Define the CNN model
model <- keras_model_sequential() 
model %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu', 
                input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 10, activation = 'softmax')
```

This CNN has two convolutional layers with 32 and 64 filters, respectively, followed by max pooling layers, a flatten layer, two dense layers (one with dropout), and a softmax activation function in the output layer for multiclass classification.

```{r}
# Compile the model
model %>% compile(
  loss = 'categorical_crossentropy', 
  optimizer = 'adam', 
  metrics = c('accuracy')
)
```

```{r}
# Define custom callback function to print metrics every n batches
print_callback <- callback_lambda(
  on_batch_end = function(batch, logs) {
    if (batch %% 50 == 0) { # Print every 50 batches
      message(paste0("Batch ", batch, ", loss: ", logs$loss, ", acc: ", logs$acc))
    }
  }
)
```

`callback_lambda` is a built-in function in the `keras` package. It is used to create custom callbacks for monitoring the training of a neural network model.

The `callback_lambda` function creates a callback that allows you to define a function that will be executed at certain points during the model training process. The function defined by the user is passed as an argument to `callback_lambda` and is executed by the callback during training.

The function defined by the user should take at least two arguments: `epoch` and `logs`. The `epoch` argument is an integer that represents the current epoch of the training process, and the `logs` argument is a list that contains the metrics calculated for the current batch, such as the loss and accuracy.

Using `callback_lambda`, you can define custom functions to do things like printing out the loss and accuracy after each epoch or batch, saving the model weights at certain checkpoints, or adjusting the learning rate based on the performance of the model.

In the example we've been discussing, the on_batch_end argument of `callback_lambda` is used to define a function that prints the loss and accuracy metrics after every 50 batches. This function is then passed as a callback to the fit method of the model, which allows it to be executed during training.

```{r}
# Train the model
history <- model %>% fit(
  df_train_x, df_train_y, 
  epochs = 5, batch_size = 128, 
  verbose = 2, # Print summary of each epoch
  # steps_per_epoch = 100, # Print summary of every 50th batch
  validation_data = list(df_test_x, df_test_y),
  callbacks = list(print_callback)
)
```

```{r}
# Evaluate the model
model %>% evaluate(df_test_x, df_test_y)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
